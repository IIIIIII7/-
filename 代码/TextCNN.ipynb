{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "D:\\Anaconda3\\envs\\tens\\lib\\site-packages\\gensim\\utils.py:862: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import jieba.posseg as pseg\n",
    "import jieba\n",
    "import re\n",
    "import string\n",
    "import csv\n",
    "import keras as layers\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from gensim.models import word2vec\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import *\n",
    "from keras.models import load_model\n",
    "from keras.models import Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#全局变量，一些参数，这样后面比较好改动\n",
    "#这几个参数都能够进行调整，以及没有使用到的参数都能用来调整模型\n",
    "# 设置词语向量的维度,这个参数暂时还不适合新闻正文的大小\n",
    "num_features = 200 #原来是100\n",
    "# 保证被拷问词语的最低频度\n",
    "min_word_count = 5\n",
    "# 设置并行化训练使用CPU计算核心数量\n",
    "num_workers = 4\n",
    "# 设置词语上下文窗口的大小\n",
    "context = 5\n",
    "#句子最大长度兼向量的维度\n",
    "maxLen=50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#去除停用词的两个函数\n",
    "def stopwordslist(stopfname): #读入停用词\n",
    "    stopwords=[line.strip() for line in open(stopfname,encoding='UTF-8').readlines()]\n",
    "    return stopwords\n",
    "\n",
    "def deleteStop(sentence,stopfname):   #根据读入的停用词，把分词后的数据中的停用词删掉\n",
    "    stopwords=stopwordslist(stopfname)\n",
    "    outstr=\"\"\n",
    "    for i in sentence:\n",
    "        if i not in stopwords and i!=\"\\n\":\n",
    "            outstr+=i\n",
    "    return outstr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#进行分词用的函数\n",
    "def wordCut(Text,stopfname):#这里传的是前面的listText\n",
    "    Mat=[]\n",
    "    for rec in Text:\n",
    "        senten=[]\n",
    "        rec=re.sub('[%s]' % re.escape(string.punctuation),'',str(rec))  #去掉逗号，句号等标点符号\n",
    "        fenci=jieba.lcut(rec)    #精准模式分词\n",
    "        stc=deleteStop(fenci,stopfname)    #去停用词\n",
    "        seg_list=pseg.cut(stc)   #标注词性，紧接着去掉对分类来说没用的词，比如人名地名等\n",
    "        for word,flag in seg_list:\n",
    "            if flag not in [\"nr\",\"ns\",\"nt\",\"nz\",\"m\",\"f\",\"ul\",\"l\",\"r\",\"t\"]:\n",
    "                senten.append(word)\n",
    "        Mat.append(senten)\n",
    "    return Mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#读取数据，并且划分出来训练集和测试集\n",
    "\n",
    "#读取数据\n",
    "data_path=\"./data.txt\"\n",
    "text=[]\n",
    "with open(data_path,\"r\",encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        line.strip()   #去掉换行符\n",
    "        text.append(line)\n",
    "text.pop(0)     #把label和text那行字符去掉，直接处理\n",
    "\n",
    "#划分测试集和训练集，data本身一共7766条数据，其中5322条积极的，2444条消极的，决定训练集：测试集=8：2进行分配\n",
    "#故共选择4256条积极，1952条消极作为训练集(6208条)，其余作为测试集（1558条）\n",
    "train_data=[]\n",
    "test_data=[]\n",
    "#生成训练集\n",
    "for i in range(0,4256):\n",
    "    train_data.append(text[i])\n",
    "for i in range(5322,7274):\n",
    "    train_data.append(text[i])\n",
    "#生成测试集\n",
    "for i in range(4256,5322):\n",
    "    test_data.append(text[i])\n",
    "for i in range(7274,7766):\n",
    "    test_data.append(text[i])\n",
    "    \n",
    "random.shuffle(train_data)  #打乱数据，以防止之后训练模型的时候效果不好\n",
    "random.shuffle(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#使用分词函数，并去除停用词\n",
    "\n",
    "#先将标签和文本分开\n",
    "train_text=[]\n",
    "test_text=[]\n",
    "train_label=[]\n",
    "test_label=[]\n",
    "#训练集\n",
    "for line in train_data:\n",
    "    temp=line.split(',')\n",
    "    train_label.append(temp[0])\n",
    "    train_text.append(temp[1].strip()) #之前的换行符没去干净，这一步又去掉了一下\n",
    "#测试集\n",
    "for line in test_data:\n",
    "    temp=line.split(',')\n",
    "    test_label.append(temp[0])\n",
    "    test_text.append(temp[1].strip()) #之前的换行符没去干净，这一步又去掉了一下"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\AAAAA\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 0.670 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    }
   ],
   "source": [
    "#对文本进行分词，训练集和测试集都需要\n",
    "train_cut=wordCut(train_text,\"./stopword.txt\") #数据类型是二维list，一行是一个list，每个词组是其中一个元素\n",
    "test_cut=wordCut(test_text,\"./stopword.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#训练集\n",
    "train_ccut=[]\n",
    "test_ccut=[]\n",
    "for line in train_cut:\n",
    "    st=\"\"\n",
    "    for j in line:\n",
    "        st+=j\n",
    "        st+=\" \"\n",
    "    train_ccut.append(st)\n",
    "# print(type(train_ccut))   #数据类型是list，每一个元素是一句话的分词结果，并且用空格隔开\n",
    "\n",
    "#测试集\n",
    "for line in test_cut:\n",
    "    st=\"\"\n",
    "    for j in line:\n",
    "        st+=j\n",
    "        st+=\" \"\n",
    "    test_ccut.append(st)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#数据预处理\n",
    "#标签，使用LabelEncoder对数据标签进行规格化处理\n",
    "def encodeLabel(data):\n",
    "    #规格化处理\n",
    "    le=LabelEncoder()\n",
    "    resultLabel=le.fit_transform(data)#这里把label变成了数字的形式\n",
    "    return resultLabel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#fit_on_texts函数可以将输入的文本中的每个词编号，编号是根据词频的，词频越大，编号越小\n",
    "#给每个词按照词频进行编号，返回编号后的“词典”\n",
    "def getNum(word):\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(word)\n",
    "    vocab = tokenizer.word_index  # 得到每个词的编号，这里的vocab已剔除掉stoplist了\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#word2vec的训练,将训练好的模型写入filename中，后面可以直接拿出来进行使用\n",
    "#参数中的words是用来训练模型的分好词后的数据\n",
    "def w2v(filename,words):\n",
    "    # word2vec模型建立\n",
    "    model=word2vec.Word2Vec(words,sg=1,size=num_features,window=5,min_count=5,negative=3,sample=0.001,hs=1,workers=4)\n",
    "    # 强制单位归一化\n",
    "    model.init_sims(replace=True)\n",
    "    # 输入一个路径，保存模型\n",
    "    model.save(filename)\n",
    "    # model.wv.save_word2vec_format('这里写一个文件路径和名字',binary=False)  #这里是存下来模型自己可以看看的，好像是这样"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#正文内容的编码\n",
    "#特征数字编号，不足在前面补0\n",
    "def getCode_Text(ww):\n",
    "    tokenizer = Tokenizer()\n",
    "    tID = tokenizer.texts_to_sequences(ww)\n",
    "    tSeq = pad_sequences(tID, maxlen=maxLen)\n",
    "    return tSeq\n",
    "\n",
    "#标签的热独码,返回编好码的数据\n",
    "def getCode_Label(label):\n",
    "    tCate = to_categorical(label)  # 将标签转换为ont-hot编码,有10种，所以参数传的10\n",
    "    return tCate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from numpy import nan as NaN\n",
    "#一些数据处理相关的东西\n",
    "word=[]\n",
    "for i in train_ccut:\n",
    "    word.append(i)\n",
    "for j in test_ccut:\n",
    "    word.append(j)\n",
    "\n",
    "#这个地方是去除word中空的数据，不然之后使用Tokenizer时会因为存在空数据而报错\n",
    "count=0\n",
    "for i in word:\n",
    "#     print(i)\n",
    "    if i is \"\":\n",
    "        word.pop(count)\n",
    "    count+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 把正文内容处理为向量，直接使用Tokenizer提供的函数进行实现\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(word)\n",
    "vocab = tokenizer.word_index\n",
    "trainID = tokenizer.texts_to_sequences(train_ccut)\n",
    "traindata = pad_sequences(trainID, maxlen=maxLen)\n",
    "testID = tokenizer.texts_to_sequences(test_ccut)\n",
    "testdata = pad_sequences(testID, maxlen=maxLen)\n",
    "# 把标签处理为数字，使用现成的库中的函数进行实现的\n",
    "trainL=encodeLabel(train_label)\n",
    "testL=encodeLabel(test_label)\n",
    "trl=getCode_Label(trainL)\n",
    "tel=getCode_Label(testL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#这下面就是模型的东西了\n",
    "#首先是word2vec模型\n",
    "model=word2vec.Word2Vec(word,sg=1,size=num_features,window=5,min_count=5,negative=3,sample=0.001,hs=1,workers=4)\n",
    "model.init_sims(replace=True)\n",
    "    # 输入一个路径，保存模型\n",
    "model.save('./w2vmodel')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#这下面就是模型的东西了\n",
    "#其次是cnn相关的\n",
    "from keras.models import Model\n",
    "w2v_model = word2vec.Word2Vec.load('./w2vmodel') #取出之前保存的word2vec模型\n",
    "embedding_matrix = np.zeros((len(vocab) + 1, num_features))  # 前面的vacab是前面编号好的那些，后面的参数是表示维度是100维\n",
    "for word, i in vocab.items():\n",
    "    try:  # 防止有没有找到的词时出错\n",
    "        embedding_vector = w2v_model[str(word)]\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "    except KeyError:\n",
    "        continue\n",
    "\n",
    "# 模型结构：词嵌入-卷积池化*3-拼接-全连接-dropout-全连接\n",
    "main_input = Input(shape=(maxLen,), dtype='float64')\n",
    "# 词嵌入（使用预训练的词向量）\n",
    "embedder = Embedding(len(vocab) + 1, num_features, input_length=maxLen, weights=[embedding_matrix], trainable=False)\n",
    "#embedder = Embedding(len(vocab) + 1, 300, input_length=50, trainable=False)\n",
    "embed = embedder(main_input)\n",
    "# 词窗大小分别为1,2,3\n",
    "cnn1 = Conv1D(256, 3, padding='same', strides=1, activation='relu')(embed)\n",
    "cnn1 = MaxPooling1D(pool_size=38)(cnn1)\n",
    "cnn2 = Conv1D(256, 4, padding='same', strides=1, activation='relu')(embed)\n",
    "cnn2 = MaxPooling1D(pool_size=37)(cnn2)\n",
    "cnn3 = Conv1D(256, 5, padding='same', strides=1, activation='relu')(embed)\n",
    "cnn3 = MaxPooling1D(pool_size=36)(cnn3)\n",
    "\n",
    "#新加几个卷积层和池化层\n",
    "# cnn4 = Conv1D(256, 6, padding='same', strides=1, activation='relu')(embed)\n",
    "# cnn4 = MaxPooling1D(pool_size=34)(cnn4)\n",
    "# cnn5 = Conv1D(256, 7, padding='same', strides=1, activation='relu')(embed)\n",
    "# cnn5 = MaxPooling1D(pool_size=35)(cnn5)\n",
    "# cnn6 = Conv1D(256, 8, padding='same', strides=1, activation='relu')(embed)\n",
    "# cnn6 = MaxPooling1D(pool_size=36)(cnn6)\n",
    "\n",
    "\n",
    "# 合并三个模型的输出向量\n",
    "cnn = concatenate([cnn1,cnn2,cnn3], axis=-1)\n",
    "flat = Flatten()(cnn)\n",
    "drop = Dropout(0.2)(flat)  #这里是dropout层，目的是防止过拟合\n",
    "main_output = Dense(units=2, activation='softmax')(drop)\n",
    "model = Model(inputs=main_input, outputs=main_output)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4966 samples, validate on 1242 samples\n",
      "Epoch 1/20\n",
      "4966/4966 [==============================] - 13s - loss: 0.6666 - acc: 0.6422 - val_loss: 0.6341 - val_acc: 0.7061\n",
      "Epoch 2/20\n",
      "4966/4966 [==============================] - 12s - loss: 0.6105 - acc: 0.7358 - val_loss: 0.5980 - val_acc: 0.7134\n",
      "Epoch 3/20\n",
      "4966/4966 [==============================] - 12s - loss: 0.5722 - acc: 0.7447 - val_loss: 0.5803 - val_acc: 0.7174\n",
      "Epoch 4/20\n",
      "4966/4966 [==============================] - 12s - loss: 0.5459 - acc: 0.7479 - val_loss: 0.5703 - val_acc: 0.7230\n",
      "Epoch 5/20\n",
      "4966/4966 [==============================] - 12s - loss: 0.5300 - acc: 0.7573 - val_loss: 0.5701 - val_acc: 0.7246\n",
      "Epoch 6/20\n",
      "4966/4966 [==============================] - 12s - loss: 0.5203 - acc: 0.7642 - val_loss: 0.5668 - val_acc: 0.7311\n",
      "Epoch 7/20\n",
      "4966/4966 [==============================] - 12s - loss: 0.5109 - acc: 0.7763 - val_loss: 0.5669 - val_acc: 0.7303\n",
      "Epoch 8/20\n",
      "4966/4966 [==============================] - 13s - loss: 0.5023 - acc: 0.7811 - val_loss: 0.5629 - val_acc: 0.7311\n",
      "Epoch 9/20\n",
      "4966/4966 [==============================] - 14s - loss: 0.4936 - acc: 0.7870 - val_loss: 0.5640 - val_acc: 0.7311\n",
      "Epoch 10/20\n",
      "4966/4966 [==============================] - 13s - loss: 0.4868 - acc: 0.7930 - val_loss: 0.5630 - val_acc: 0.7327\n",
      "Epoch 11/20\n",
      "4966/4966 [==============================] - 13s - loss: 0.4796 - acc: 0.7960 - val_loss: 0.5650 - val_acc: 0.7295\n",
      "Epoch 12/20\n",
      "4966/4966 [==============================] - 13s - loss: 0.4707 - acc: 0.8043 - val_loss: 0.5708 - val_acc: 0.7271\n",
      "Epoch 13/20\n",
      "4966/4966 [==============================] - 13s - loss: 0.4638 - acc: 0.8053 - val_loss: 0.5698 - val_acc: 0.7319\n",
      "Epoch 14/20\n",
      "4966/4966 [==============================] - 13s - loss: 0.4578 - acc: 0.8087 - val_loss: 0.5730 - val_acc: 0.7303\n",
      "Epoch 15/20\n",
      "4966/4966 [==============================] - 12s - loss: 0.4497 - acc: 0.8115 - val_loss: 0.5717 - val_acc: 0.7311\n",
      "Epoch 16/20\n",
      "4966/4966 [==============================] - 12s - loss: 0.4431 - acc: 0.8182 - val_loss: 0.5723 - val_acc: 0.7327\n",
      "Epoch 17/20\n",
      "4966/4966 [==============================] - 12s - loss: 0.4382 - acc: 0.8210 - val_loss: 0.5801 - val_acc: 0.7271\n",
      "Epoch 18/20\n",
      "4966/4966 [==============================] - 12s - loss: 0.4314 - acc: 0.8234 - val_loss: 0.5759 - val_acc: 0.7303\n",
      "Epoch 19/20\n",
      "4966/4966 [==============================] - 12s - loss: 0.4256 - acc: 0.8232 - val_loss: 0.5753 - val_acc: 0.7319\n",
      "Epoch 20/20\n",
      "4966/4966 [==============================] - 12s - loss: 0.4205 - acc: 0.8294 - val_loss: 0.5867 - val_acc: 0.7238\n"
     ]
    }
   ],
   "source": [
    "history=model.fit(traindata, trl, batch_size=800, epochs=20,validation_split=0.2) #训练模型，分批进行训练，一共训练20次，每次800条数据，预留20%的数据用于对当前的模型进行测试\n",
    "model.save(\"./textCNNmodel\") #保存模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 1 ..., 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "result = model.predict(testdata)  # 预测样本属于每个类别的概率\n",
    "result_labels = np.argmax(result, axis=1)  # 获得最大概率对应的标签\n",
    "print(result_labels)\n",
    "y_predict = list(map(str, result_labels))#把预测结果转变为list型的数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "准确率 0.713735558408\n"
     ]
    }
   ],
   "source": [
    "#计算准确率\n",
    "from sklearn.metrics import accuracy_score\n",
    "print('准确率', accuracy_score(test_label, y_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
